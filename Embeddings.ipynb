{"cells":[{"cell_type":"markdown","source":["\n","# üß† **Understanding Embeddings with BERT**\n","\n","In this notebook, we go into the world of embeddings, utilizing the BERT model to understand the semantic relationships between words in different contexts.\n","\n","## üõ†Ô∏è Setup and Installation\n","\n","Start by installing the necessary libraries to ensure all functionalities are available."],"metadata":{"id":"yUuby3g8ouzm"}},{"cell_type":"code","source":["!pip install transformers==4.29.2\n","!pip install scipy==1.7.3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6C1auKaP5I93","executionInfo":{"status":"ok","timestamp":1713953043725,"user_tz":-330,"elapsed":21054,"user":{"displayName":"Harshini Chandak","userId":"15644444896488863717"}},"outputId":"9920cc39-c93d-4e8d-f1ec-390c3147dc60"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.29.2 in /usr/local/lib/python3.10/dist-packages (4.29.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.2) (2024.2.2)\n","Requirement already satisfied: scipy==1.7.3 in /usr/local/lib/python3.10/dist-packages (1.7.3)\n","Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from scipy==1.7.3) (1.22.4)\n"]}]},{"cell_type":"markdown","source":["## üìö Importing Libraries\n","\n","Import essential modules for our tasks."],"metadata":{"id":"mpAjguA9o7tT"}},{"cell_type":"code","source":["from transformers import BertModel, AutoTokenizer\n","from scipy.spatial.distance import cosine"],"metadata":{"id":"DBwmRlXP5zY6","executionInfo":{"status":"ok","timestamp":1713953047339,"user_tz":-330,"elapsed":3618,"user":{"displayName":"Harshini Chandak","userId":"15644444896488863717"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## ü§ñ Model Setup\n","\n","Load the pre-trained BERT model and tokenizer. This model will help us extract embeddings for our analysis."],"metadata":{"id":"XsfQ55vto_Bd"}},{"cell_type":"code","source":["# Defining the model name\n","model_name = \"bert-base-cased\"\n","\n","# Loading the pre-trained model and tokenizer\n","model = BertModel.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-JQ5YWeJB6Y","executionInfo":{"status":"ok","timestamp":1713953051142,"user_tz":-330,"elapsed":3809,"user":{"displayName":"Harshini Chandak","userId":"15644444896488863717"}},"outputId":"5b24c9a0-2b86-4cd5-96d2-41d315a65c5c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","source":["\n","## üìù Function Definition: Predict\n","\n","Define a function that encodes input text into tensors, which are then fed to the model to obtain embeddings."],"metadata":{"id":"ZzoQD4KrpClk"}},{"cell_type":"code","source":["# Defining a function to encode the input text and get model predictions\n","def predict(text):\n","    encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n","    return model(**encoded_inputs)[0]"],"metadata":{"id":"GJIJ1VUZJBtp","executionInfo":{"status":"ok","timestamp":1713953051142,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harshini Chandak","userId":"15644444896488863717"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## üìÉ Defining the Sentences\n","\n","Set up sentences to analyze. The"],"metadata":{"id":"eGyuVbYkpEfU"}},{"cell_type":"code","source":["# Defining the sentences\n","sentence1 = \"There was a fly drinking from my soup\"\n","sentence2 = \"There is a fly swimming in my juice\"\n","# sentence2 = \"To become a commercial pilot, he had to fly for 1500 hours.\" # second fly example\n","\n","# Tokenizing the sentences\n","tokens1 = tokenizer.tokenize(sentence1)\n","tokens2 = tokenizer.tokenize(sentence2)"],"metadata":{"id":"s_0dKdRbJFtL","executionInfo":{"status":"ok","timestamp":1713953051142,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harshini Chandak","userId":"15644444896488863717"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## üîç Tokenization and Model Predictions\n","\n","Tokenize the sentences and obtain predictions (embeddings) from the model."],"metadata":{"id":"fTn6NoNepG16"}},{"cell_type":"code","source":["# Getting model predictions for the sentences\n","out1 = predict(sentence1)\n","out2 = predict(sentence2)"],"metadata":{"id":"5B0LO9xfJI2P","executionInfo":{"status":"ok","timestamp":1713953051143,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harshini Chandak","userId":"15644444896488863717"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## üîÑ Extracting Embeddings\n","\n","Extract embeddings specifically for the word \"fly\" from both sentences."],"metadata":{"id":"Mb8tPoc3pJxA"}},{"cell_type":"code","source":["# Extracting embeddings for the word 'fly' in both sentences\n","emb1 = out1[0:, tokens1.index(\"fly\"), :].detach()[0]\n","emb2 = out2[0:, tokens2.index(\"fly\"), :].detach()[0]\n","\n","# emb1 = out1[0:, 3, :].detach()\n","# emb2 = out2[0:, 3, :].detach()"],"metadata":{"id":"y8ZvAkGiJLZf","executionInfo":{"status":"ok","timestamp":1713953051143,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harshini Chandak","userId":"15644444896488863717"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## üìä Calculating Cosine Similarity\n","\n","Calculate the cosine similarity between the embeddings of the word \"fly\" from both sentences to measure how context affects meaning."],"metadata":{"id":"gcvcIGiOpLyr"}},{"cell_type":"code","source":["# Calculating the cosine similarity between the embeddings\n","cosine(emb1, emb2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKVSZUxyJNZs","executionInfo":{"status":"ok","timestamp":1713953051143,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harshini Chandak","userId":"15644444896488863717"}},"outputId":"2758bce4-37e2-4276-fb49-c1c18d66949e"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.06798791885375977"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## üåü Conclusion\n","\n","This notebook has guided you through the process of extracting and comparing word embeddings using BERT. Such techniques are fundamental in understanding word semantics and their usage across different contexts.\n","\n","Experiment by changing the sentences or focusing on different words to see how the embeddings and their similarities vary!"],"metadata":{"id":"gfFR3mbspN8y"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1bQLidcWx-dj8SH1bCOfzB6wUpl2bso4l","timestamp":1713952766819}]},"kernelspec":{"name":"python3","display_name":"Python 3"}}}